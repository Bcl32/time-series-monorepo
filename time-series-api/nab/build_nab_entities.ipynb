{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('/app/python-modules')\n",
    "\n",
    "!{sys.executable} -m pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.time_function import time_function as timeit\n",
    "from utils.TimeSeriesClient import TimeSeriesClient\n",
    "\n",
    "#custom imports\n",
    "import config\n",
    "import nab_utils\n",
    "\n",
    "from time import perf_counter\n",
    "client = TimeSeriesClient(base_url='http://127.0.0.1:8000', timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_path=Path(config.ROOT_PATH) / \"static\" / \"nab\" / \"raw_datasets\"\n",
    "collection_folders=next(os.walk(collections_path))[1] #GET ALL MODEL FOLDERS\n",
    "\n",
    "collection_descriptions={}\n",
    "collection_descriptions[\"realAWSCloudwatch\"] = \"AWS server metrics as collected by the AmazonCloudwatch service. Example metrics include CPU Utilization, Network Bytes In, and Disk Read Bytes.\"\n",
    "collection_descriptions[\"realAdExchange\"] = \"Online advertisement clicking rates, where the metrics are cost-per-click (CPC) and cost per thousand impressions (CPM). One of the files is normal, without anomalies.\"\n",
    "collection_descriptions[\"realKnownCause\"] = \"This is data for which we know the anomaly causes; no hand labeling.\"\n",
    "collection_descriptions[\"realTraffic\"] = \"Real time traffic data from the Twin Cities Metro area in Minnesota, collected by the Minnesota Department of Transportation. Included metrics include occupancy, speed, and travel time from specific sensors.\"\n",
    "collection_descriptions[\"realTweets\"] = \"A collection of Twitter mentions of large publicly-traded companies such as Google and IBM. The metric value represents the number of mentions for a given ticker symbol every 5 minutes.\"\n",
    "collection_descriptions[\"artificialNoAnomaly\"] = \"Artificially-generated data without any anomalies.\"\n",
    "collection_descriptions[\"artificialWithAnomaly\"] = \"Artificially-generated data with varying types of anomalies.\"\n",
    "\n",
    "def build_collection_object(collection_name):\n",
    "    collection_object={}\n",
    "    collection_object[\"name\"]=collection_name\n",
    "    collection_object[\"description\"]=collection_descriptions[collection_name]\n",
    "    collection_object[\"tags\"]=[\"nab\"]\n",
    "    return collection_object\n",
    "\n",
    "async def register_collections(collection_folders):\n",
    "    collections=[]\n",
    "    for collection_name in collection_folders:\n",
    "        collection_object=build_collection_object(collection_name)#build collection object\n",
    "        collections.append(collection_object)\n",
    "\n",
    "    collection_data=await client.request(\"post\",\"/collection/create_many\", json=collections)\n",
    "    \n",
    "    return [{\"name\":entry['name'],\"id\":entry['id']} for entry in collection_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Labels and Set up labeler detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels file for anomalies\n",
    "LABELS_FILE=Path(config.NAB_ASSETS) / \"labels\" / \"combined_labels.json\"\n",
    "with open(LABELS_FILE) as json_file:\n",
    "    labels=json.load(json_file)\n",
    "\n",
    "async def register_original_dectector():\n",
    "    detector_object={}\n",
    "    detector_object[\"name\"] = \"labeler\"\n",
    "    detector_object[\"description\"] = \"A collection of the original datasets with the provided ground truth labels for anomalies.\"\n",
    "    detector_object[\"source\"] = \"https://github.com/numenta/NAB\"\n",
    "    detector_object[\"documentation\"] = \"https://github.com/numenta/NAB/wiki#nab-whitepaper\"\n",
    "    detector_object[\"tags\"] = ['Demo']\n",
    "\n",
    "    detector_data = await client.request(\"post\",\"/detector/create\", json={\"payload\":detector_object})\n",
    "    return detector_data\n",
    "\n",
    "detector_obj = await register_original_dectector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to build datafeed, dataset, health and anomaly objects for a specific datafeed, adds a dataset for each one that is the file representing the dataset, and all the anomalies which are later added to a prediction object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_objects(dataset_name):\n",
    "    \"\"\" Starting with a datafeed object, load the dataset it represents, then generate a health and dataset object for each dataset and add as children during dataset generation\n",
    "    then, also builds all anomaly objects which are later added to a prediction object in another function, assigned to the labeler detector that has anomalies for each ground truth label\"\"\"\n",
    "    datafeed_object={}\n",
    "\n",
    "    #load dataset and format\n",
    "    dataset=pd.read_csv(config.NAB_ASSETS / \"raw_datasets\" / dataset_name) #load dataset\n",
    "    dataset['timestamp']=pd.to_datetime(dataset['timestamp']) #set timestamps to datetime objects\n",
    "    dataset=dataset.set_index(\"timestamp\") #set index to datetime column\n",
    "    dataset[\"anomaly_label\"]=False # creates new column for labeled anomalies and fills with False\n",
    "    nab_utils.add_labels_to_dataset(dataset,dataset_name)\n",
    "\n",
    "    #build dataset model object\n",
    "    datafeed_object[\"name\"]=dataset_name.split(\"/\")[1].replace('_', ' ').title()[:-4] #takes a string, replaces underscores with spaces, and capitalizes each word, then removes .csv from the end of the filename\n",
    "    datafeed_object[\"folder\"] = dataset_name.split(\"/\")[0]\n",
    "    datafeed_object[\"filename\"] = dataset_name.split(\"/\")[1]\n",
    "    \n",
    "    datafeed_object[\"feed_type\"] = 'file'\n",
    "    datafeed_object[\"start_time\"] = dataset.index[0].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    datafeed_object[\"end_time\"] = dataset.index[1].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    datafeed_object[\"anomaly_count\"] = len(dataset[dataset['anomaly_label'] == True])\n",
    "\n",
    "    health_object={}\n",
    "    health_object[\"heartbeat_frequency\"] = 3600\n",
    "    health_object[\"heartbeat_timeout\"] = 3\n",
    "    health_object[\"last_received\"] = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    health_object[\"score\"]=100\n",
    "    health_object[\"status\"]=\"active\"\n",
    "\n",
    "    # datafeed_object[\"health\"]=health_object\n",
    "\n",
    "    dataset_object={}\n",
    "    dataset_object[\"anomaly_count\"] = len(dataset[dataset['anomaly_label'] == True])\n",
    "    dataset_object[\"labeled\"] = 'True'\n",
    "    dataset_object[\"file_type\"] = 'csv'\n",
    "    dataset_object[\"path\"] = dataset_name\n",
    "    dataset_object[\"folder\"] = dataset_name.split(\"/\")[0]\n",
    "    dataset_object[\"filename\"] = dataset_name.split(\"/\")[1]\n",
    "    dataset_object[\"start_time\"] = dataset.index[0].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    dataset_object[\"end_time\"] = dataset.index[1].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "\n",
    "\n",
    "    if 'real' in dataset_name:\n",
    "        datafeed_object[\"tags\"]=['real']\n",
    "        dataset_object[\"tags\"]=['real']\n",
    "    else:\n",
    "        datafeed_object[\"tags\"]=['artificial']\n",
    "        dataset_object[\"tags\"]=['real']\n",
    "\n",
    "    # datafeed_object[\"dataset\"]=dataset_object\n",
    "\n",
    "    anomaly_records=[]\n",
    "    #build anomaly records for this dataset\n",
    "    for item in labels[dataset_name]: #for each entry in the labels for this file\n",
    "        anomaly_object={}\n",
    "        anomaly_object[\"dataset_name\"] = dataset_name\n",
    "        anomaly_object[\"detector_id\"]=detector_obj['id']\n",
    "        anomaly_object[\"detector_name\"] = detector_obj['name']\n",
    "        anomaly_object[\"value\"] = float(dataset.at[item,\"value\"])\n",
    "        anomaly_object[\"anomaly_score\"] = float(0)\n",
    "        anomaly_object[\"threshold\"] = float(0)\n",
    "        anomaly_object[\"time\"] = item\n",
    "        anomaly_object[\"status\"] = 'test'\n",
    "        anomaly_object[\"severity\"] = \"low\"\n",
    "        anomaly_object[\"tags\"] = ['label']\n",
    "        anomaly_records.append(anomaly_object)\n",
    "\n",
    "    return datafeed_object, dataset_object, health_object, anomaly_records\n",
    "\n",
    "\n",
    "def build_prediction_object(dataset_obj,detector_obj):\n",
    "    \"\"\" has to be called after the other objects are made as it needs the dataset ID\"\"\"\n",
    "    prediction_object={}\n",
    "    prediction_object[\"dataset_id\"]=dataset_obj['id']\n",
    "    prediction_object[\"dataset_name\"]=dataset_obj['path']\n",
    "    prediction_object[\"detector_name\"]=detector_obj['name']\n",
    "    prediction_object[\"detector_id\"]=detector_obj['id']\n",
    "\n",
    "    prediction_object[\"url\"]= str(Path(config.NAB_ASSETS / \"predictions\" / prediction_object[\"detector_name\"]) / dataset_obj[\"folder\"] / Path(prediction_object[\"detector_name\"]+\"_\"+dataset_obj[\"filename\"]))\n",
    "    \n",
    "    return prediction_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build  / Datafeeds / Datasets / Precitions / Anomalies\n",
    "\n",
    "### Outer Loop: process_collection: for each collection object, make a datafeed object for each file in the folder, runs as a ansyncio gather for register_datafeed\n",
    "### Inner Loop: register_datafeed:  \n",
    "* Each datafeed build the object, dataset, anomaly objects using the dataset name to load the csv file and generate all the object attributes\n",
    "* Registers the datafeed with the dataset and health objects added in the same call \n",
    "* Generates a prediction object for the dataset, and adds all the anomalies in the prediction create call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "async def process_collection(collection_name, collection_id):\n",
    "    csv_files=Path(collections_path / collection_name).glob('*.csv')\n",
    "    datafeeds = await asyncio.gather(*[register_datafeed(filename,collection_id) for filename in csv_files])\n",
    "    return datafeeds\n",
    "\n",
    "async def register_datafeed(filename,collection_id):\n",
    "    dataset_name=str(Path(filename.parent.name) / filename.name)\n",
    "    datafeed_data, dataset_data, health_data, anomaly_entries=build_objects(dataset_name)\n",
    "\n",
    "    #make datafeed object, pass in health and dataset objects as children\n",
    "    response = await client.request(\"post\",\"/datafeed/create/\"+collection_id, json={\"payload\":datafeed_data,\"datasets\":[dataset_data],\"health\":health_data})\n",
    "\n",
    "    dataset_object=response[\"datasets\"][0] #get dataset (will only ever have one with this process)\n",
    "    prediction=build_prediction_object(dataset_object,detector_obj)\n",
    "\n",
    "    #make prediction object, pass in anomalies as children\n",
    "    response = await client.request(\"post\",\"/prediction/create\", json={\"payload\":prediction,\"anomalies\":anomaly_entries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_collection.time\n",
      "process_collection.time\n",
      "process_collection.time\n",
      "process_collection.time\n",
      "process_collection.time\n",
      "process_collection.time\n",
      "process_collection.time\n",
      ">>> 14.561225414276123\n",
      ">>> 14.596603631973267\n",
      ">>> 14.599840879440308\n",
      ">>> 14.780881643295288\n",
      ">>> 14.928279638290405\n",
      ">>> 14.94666838645935\n",
      ">>> 14.956026315689087\n"
     ]
    }
   ],
   "source": [
    "collections = await register_collections(collection_folders)\n",
    "#collections = await register_collections([\"realAdExchange\"])\n",
    "datafeeds = await asyncio.gather(*[process_collection(item[\"name\"],item[\"id\"]) for item in collections])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
