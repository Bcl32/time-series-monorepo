{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/app/nab\n",
      "ROOT_PATH = /app\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "!{sys.executable} -m pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#custom imports\n",
    "import config\n",
    "import nab_utils\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "BASE_URL='http://127.0.0.1:8000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def timeit(func):\n",
    "    async def process(func, *args, **params):\n",
    "        if asyncio.iscoroutinefunction(func):\n",
    "            #print('this function is a coroutine: {}'.format(func.__name__))\n",
    "            return await func(*args, **params)\n",
    "        else:\n",
    "            print('this is not a coroutine')\n",
    "            return func(*args, **params)\n",
    "\n",
    "    async def helper(*args, **params):\n",
    "        print('{}.time'.format(func.__name__))\n",
    "        start = time.time()\n",
    "        result = await process(func, *args, **params)\n",
    "        print('>>>', time.time() - start)\n",
    "        return result\n",
    "\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafeeds_path=Path(config.ROOT_PATH) / \"static\" / \"nab\" / \"datasets\"\n",
    "datafeed_folders=next(os.walk(datafeeds_path))[1] #GET ALL MODEL FOLDERS\n",
    "\n",
    "#get labels file for anomalies\n",
    "LABELS_FILE=Path(config.NAB_ASSETS) / \"labels\" / \"combined_labels.json\"\n",
    "with open(LABELS_FILE) as json_file:\n",
    "    labels=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datafeed_object(datafeed_name):\n",
    "    datafeed_object={}\n",
    "    datafeed_object[\"name\"]=datafeed_name\n",
    "    datafeed_object[\"group\"]=\"nab\"\n",
    "    datafeed_object[\"type\"]=\"demo\"\n",
    "    datafeed_object[\"url\"]=\"/nab/\"+str(datafeed_name)\n",
    "    return datafeed_object\n",
    "\n",
    "def build_dataset_object(dataset_name):\n",
    "    dataset_object={}\n",
    "\n",
    "    #load dataset and format\n",
    "    dataset=pd.read_csv(config.NAB_ASSETS / \"raw_datasets\" / dataset_name) #load dataset\n",
    "    dataset['timestamp']=pd.to_datetime(dataset['timestamp']) #set timestamps to datetime objects\n",
    "    dataset=dataset.set_index(\"timestamp\") #set index to datetime column\n",
    "    dataset[\"anomaly_label\"]=False # creates new column for labeled anomalies and fills with False\n",
    "    nab_utils.add_labels_to_dataset(dataset,dataset_name)\n",
    "\n",
    "    if 'real' in dataset_name:\n",
    "        dataset_object[\"tags\"]=['real']\n",
    "    else:\n",
    "        dataset_object[\"tags\"]=['artificial']\n",
    "\n",
    "    #build dataset model object\n",
    "    dataset_object[\"name\"]=dataset_name\n",
    "    dataset_object[\"folder\"] = dataset_name.split(\"/\")[0]\n",
    "    dataset_object[\"filename\"] = dataset_name.split(\"/\")[1]\n",
    "    dataset_object[\"labeled\"] = 'True'\n",
    "    dataset_object[\"start_time\"] = dataset.index[0].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    dataset_object[\"end_time\"] = dataset.index[1].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    dataset_object[\"anomaly_count\"] = len(dataset[dataset['anomaly_label'] == True])\n",
    "\n",
    "\n",
    "    anomaly_records=[]\n",
    "    #build anomaly records for this dataset\n",
    "    for item in labels[dataset_name]: #for each entry in the labels for this file\n",
    "        anomaly_object={}\n",
    "        anomaly_object[\"value\"] = float(dataset.at[item,\"value\"])\n",
    "        anomaly_object[\"time\"] = item\n",
    "        anomaly_object[\"model\"] = \"Labeled Anomaly\"\n",
    "        anomaly_object[\"status\"] = 'test'\n",
    "        anomaly_object[\"severity\"] = \"low\"\n",
    "        anomaly_object[\"tags\"] = ['label']\n",
    "        anomaly_records.append(anomaly_object)\n",
    "\n",
    "    return dataset_object, anomaly_records\n",
    "\n",
    "async def register_original_dectector():\n",
    "    detector_object={}\n",
    "    detector_object[\"name\"] = \"Original Labeler\"\n",
    "    detector_object[\"description\"] = \"A collection of the original datasets with the provided ground truth labels for anomalies.\"\n",
    "    detector_object[\"tags\"] = ['Demo']\n",
    "\n",
    "    async with httpx.AsyncClient(base_url=BASE_URL) as client:\n",
    "        response = await client.post(\"/detector/create\", json=detector_object)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test adding datafeed\n",
    "@timeit\n",
    "async def add_datafeed_test(datafeed_name):\n",
    "       datafeed_object=build_datafeed_object(datafeed_name)\n",
    "       async with httpx.AsyncClient(base_url=BASE_URL) as client:\n",
    "              response = await client.post(\"/datafeed/create\", json=datafeed_object)\n",
    "       return response.json()\n",
    "       \n",
    "@timeit\n",
    "async def add_dataset_test(datafeed_id):\n",
    "       #test adding a dataset with the id of a created datafeed\n",
    "       dataset_name=\"realKnownCause/nyc_taxi.csv\"\n",
    "       dataset_object,anomaly_records=build_dataset_object(dataset_name)\n",
    "       async with httpx.AsyncClient(base_url=BASE_URL) as client:\n",
    "              response = await client.post(\"/dataset/create/\"+datafeed_id, json=dataset_object)\n",
    "       return response.json()['id'], anomaly_records\n",
    "\n",
    "@timeit\n",
    "async def test_sequential_anomaly_insert(dataset_id, anomaly_records):\n",
    "       #test adding an anomaly to a dataset\n",
    "       for item in anomaly_records:\n",
    "              async with httpx.AsyncClient(base_url=BASE_URL) as client:\n",
    "                     anomaly_response = await client.post(\"/anomaly/create/\"+dataset_id, json=item)\n",
    "\n",
    "@timeit\n",
    "async def test_gather_individual_anomaly_insert(dataset_id, anomaly_records):\n",
    "       async with httpx.AsyncClient(base_url=BASE_URL) as client:\n",
    "              anomalies = await asyncio.gather(*[client.post(\"/anomaly/create/\"+dataset_id, json=anomaly) for anomaly in anomaly_records])\n",
    "       return anomalies\n",
    "\n",
    "@timeit\n",
    "async def test_batch_insert_anomalies(dataset_id, anomaly_records):\n",
    "       async with httpx.AsyncClient(base_url=BASE_URL) as client:\n",
    "              anomaly_response = await client.post(\"/anomaly/create_many/\"+dataset_id, json=anomaly_records)\n",
    "       return anomaly_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_response= await register_original_dectector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_id=detector_response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_datafeed_test.time\n",
      ">>> 0.16153311729431152\n"
     ]
    }
   ],
   "source": [
    "response = await add_datafeed_test(\"test_nab\")\n",
    "datafeed_id=response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_dataset_test.time\n",
      ">>> 0.1858048439025879\n"
     ]
    }
   ],
   "source": [
    "dataset_id, anomaly_records = await add_dataset_test(datafeed_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sequential_anomaly_insert.time\n",
      ">>> 0.8003561496734619\n",
      "test_gather_individual_anomaly_insert.time\n",
      ">>> 0.3228011131286621\n",
      "test_batch_insert_anomalies.time\n",
      ">>> 0.1400461196899414\n"
     ]
    }
   ],
   "source": [
    "result= await test_sequential_anomaly_insert(dataset_id, anomaly_records)\n",
    "result = await test_gather_individual_anomaly_insert(dataset_id, anomaly_records)\n",
    "result = await test_batch_insert_anomalies(dataset_id, anomaly_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datafeeds / Datasets / Anomalies\n",
    "\n",
    "### Loops through each datafeed and adds all the datasets, with each dataset adding it's labeled anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:31<00:00,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.190025987998524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_before=perf_counter()\n",
    "\n",
    "\n",
    "for datafeed in tqdm(datafeed_folders): #for each datafeed\n",
    "    datafeed_object=build_datafeed_object(datafeed)#build datafeed object\n",
    "    async with httpx.AsyncClient(base_url=BASE_URL) as client: #add the datafeed to the database\n",
    "        response = await client.post(\"/datafeed/datafeeds\", json=datafeed_object)\n",
    "    datafeed_id=response.json()['id']\n",
    "\n",
    "    #for each dataset in the folder (ends in .csv)\n",
    "    for csv_file in Path(datafeeds_path / datafeed).glob('*.csv'): \n",
    "        dataset_name=str(Path(csv_file.parent.name) / csv_file.name) #convert full Path object to a '/folder/filename.csv' string\n",
    "        dataset_object, anomaly_entries=build_dataset_object(dataset_name) #build dataset object by passing in the dataset name and the datafeed id\n",
    "\n",
    "        async with httpx.AsyncClient(base_url=BASE_URL) as client: #add the dataset to the database\n",
    "            response = await client.post(\"/dataset/datasets/\"+datafeed_id, json=dataset_object)\n",
    "        \n",
    "        dataset_id=response.json()['id']\n",
    "        #for each anomaly for this dataset\n",
    "        for anomaly in anomaly_entries:\n",
    "            async with httpx.AsyncClient(base_url=BASE_URL) as client: #add the anomaly to the database\n",
    "                response = await client.post(\"/anomaly/anomalies/\"+dataset_id, json=anomaly)\n",
    "\n",
    "total_time=perf_counter()-time_before\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def register_datafeed(datafeed_name):\n",
    "    print(\"registering datafeed:\" +str(datafeed_name) )\n",
    "    datafeed_object=build_datafeed_object(datafeed_name)#build datafeed object\n",
    "    async with httpx.AsyncClient(base_url=BASE_URL) as client: #add the datafeed to the database\n",
    "        response = await client.post(\"/datafeed/create\", json=datafeed_object)\n",
    "    datafeed_id=response.json()['id']\n",
    "\n",
    "    csv_files=Path(datafeeds_path / datafeed_name).glob('*.csv')\n",
    "    datasets = await asyncio.gather(*[register_dataset(filename,datafeed_id) for filename in csv_files])\n",
    "    return datasets\n",
    "\n",
    "\n",
    "async def register_dataset(filename,datafeed_id):\n",
    "    # await asyncio.sleep(10)\n",
    "    dataset_name=str(Path(filename.parent.name) / filename.name)\n",
    "    dataset_object, anomaly_entries=build_dataset_object(dataset_name) #build dataset object by passing in the dataset name and the datafeed id\n",
    "    async with httpx.AsyncClient(base_url=BASE_URL, timeout=15) as client: #add the dataset to the database\n",
    "        response = await client.post(\"/dataset/create/\"+datafeed_id, json=dataset_object) \n",
    "        dataset_id=response.json()['id']\n",
    "        #anomalies = await asyncio.gather(*[register_anomlay(anomaly,dataset_id) for anomaly in anomaly_entries])\n",
    "        #anomalies = await asyncio.gather(*[client.post(\"/anomaly/anomalies/\"+dataset_id, json=anomaly) for anomaly in anomaly_entries])\n",
    "        anomalies = await client.post(\"/anomaly/create_many/\"+dataset_id, json=anomaly_entries)\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registering datafeed:realKnownCause\n",
      "registering datafeed:realAdExchange\n",
      "2.205112324998481\n",
      "registering datafeed:artificialNoAnomaly\n",
      "registering datafeed:realAdExchange\n",
      "1.8204953249987739\n"
     ]
    }
   ],
   "source": [
    "# SEQUENTIAL TEST\n",
    "time_before=perf_counter()\n",
    "datasets= await register_datafeed(\"realKnownCause\")\n",
    "datasets= await register_datafeed(\"realAdExchange\")\n",
    "print(perf_counter()-time_before)\n",
    "# PARALLEL TEST\n",
    "time_before=perf_counter()\n",
    "await asyncio.gather(register_datafeed(\"artificialNoAnomaly\"),register_datafeed(\"realAdExchange\"))\n",
    "print(perf_counter()-time_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registering datafeed:artificialNoAnomaly\n",
      "registering datafeed:artificialWithAnomaly\n",
      "registering datafeed:realAdExchange\n",
      "registering datafeed:realAWSCloudwatch\n",
      "registering datafeed:realKnownCause\n",
      "registering datafeed:realTraffic\n",
      "registering datafeed:realTweets\n",
      "9.44293911099885\n"
     ]
    }
   ],
   "source": [
    "# with individual anomaly create calls\n",
    "time_before=perf_counter()\n",
    "datafeeds = await asyncio.gather(*[register_datafeed(datafeed_name) for datafeed_name in datafeed_folders])\n",
    "print(perf_counter()-time_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '38bddb2a-06b0-4cc9-a434-4b33e7fc1eaa',\n",
       " 'time_created': '2024-09-06T21:42:13.546260Z',\n",
       " 'time_updated': '2024-09-06T21:42:14.400466Z',\n",
       " 'value': 40.0,\n",
       " 'time': '2014-04-11T00:00:00Z',\n",
       " 'model': 'Labeled Anomaly',\n",
       " 'status': 'status',\n",
       " 'severity': 'low',\n",
       " 'tags': ['demo'],\n",
       " 'dataset_id': '77f95f3b-2951-4367-9f84-b8ddf670a9aa'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafeeds[1][0][0].json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5c19d144-9b62-4786-abf7-d0377f33b970',\n",
       " 'time_created': '2024-09-06T20:53:16.793671Z',\n",
       " 'time_updated': '2024-09-06T20:53:16.884305Z',\n",
       " 'name': 'artificialNoAnomaly/art_daily_no_noise.csv',\n",
       " 'folder': 'artificialNoAnomaly',\n",
       " 'filename': 'art_daily_no_noise.csv',\n",
       " 'labeled': True,\n",
       " 'anomaly_count': 0,\n",
       " 'tags': ['artificial'],\n",
       " 'start_time': '2014-04-01T00:00:00Z',\n",
       " 'end_time': '2014-04-01T00:05:00Z',\n",
       " 'datafeed_id': 'c3b3d94d-8637-496c-8458-552c1a7b8e93',\n",
       " 'anomalies': []}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafeeds[0][0].json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
