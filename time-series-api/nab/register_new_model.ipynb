{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('/app/python-modules')\n",
    "\n",
    "!{sys.executable} -m pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.time_function import time_function as timeit\n",
    "\n",
    "#custom imports\n",
    "import config\n",
    "import nab_utils\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "from utils.TimeSeriesClient import TimeSeriesClient\n",
    "client = TimeSeriesClient(base_url='http://127.0.0.1:8000', timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load threhsolds\n",
    "with open(config.NAB_ASSETS / \"config\" / \"thresholds.json\") as json_file:\n",
    "    thresholds_file=json.load(json_file)\n",
    "\n",
    "#load detector metadata\n",
    "with open(\"detector_metadata.json\") as json_file:\n",
    "    detector_metadata=json.load(json_file)\n",
    "\n",
    "#get dataset IDs\n",
    "datasets = await client.request(\"get\",\"/dataset/get_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction_object(dataset_obj,detector_obj):\n",
    "    prediction_object={}\n",
    "    prediction_object[\"dataset_id\"]=dataset_obj['id']\n",
    "    prediction_object[\"dataset_name\"]=dataset_obj['path']\n",
    "    prediction_object[\"detector_name\"]=detector_obj['name']\n",
    "    prediction_object[\"detector_id\"]=detector_obj['id']\n",
    "\n",
    "    prediction_object[\"url\"]= str(Path(config.NAB_ASSETS / \"predictions\" / prediction_object[\"detector_name\"]) / dataset_obj[\"folder\"] / Path(prediction_object[\"detector_name\"]+\"_\"+dataset_obj[\"filename\"]))\n",
    "    \n",
    "    return prediction_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_anomaly_objects(results,detector_obj, prediction_obj):\n",
    "    THRESHOLD=thresholds_file[MODEL][\"standard\"][\"threshold\"]\n",
    "    results[\"is_anomaly\"]=results[\"anomaly_score\"].map(lambda x: x > THRESHOLD)\n",
    "    anomalies=[i for i,v in enumerate(results[\"is_anomaly\"]==True) if v]\n",
    "\n",
    "    anomaly_records=[]\n",
    "    #build anomaly records for this dataset\n",
    "    for anomaly_index in anomalies:\n",
    "        anomaly=results.loc[results.index[anomaly_index]]\n",
    "        anomaly_object={}\n",
    "        #anomaly_object[\"prediction_id\"]=prediction_obj[\"id\"]\n",
    "        anomaly_object[\"detector_id\"]=detector_obj[\"id\"]\n",
    "        anomaly_object[\"dataset_name\"] = prediction_obj[\"dataset_name\"]\n",
    "        anomaly_object[\"detector_name\"] = MODEL\n",
    "        anomaly_object[\"value\"] = float(anomaly[\"value\"])\n",
    "        anomaly_object[\"anomaly_score\"] = float(anomaly[\"anomaly_score\"])\n",
    "        anomaly_object[\"threshold\"] = float(THRESHOLD)\n",
    "        anomaly_object[\"time\"] = anomaly[\"timestamp\"].strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "        anomaly_object[\"status\"] = 'open'\n",
    "        anomaly_object[\"severity\"] = [element for element in detector_metadata[\"detectors\"] if element['name'] == MODEL][0][\"severity\"] #get severity from metadata model file\n",
    "        anomaly_object[\"tags\"] = ['nab']\n",
    "        anomaly_records.append(anomaly_object)\n",
    "    return anomaly_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build predictions, get anomalies and add as children to each prediction call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def register_predictions():\n",
    "    prediction_objects=[]\n",
    "    for dataset in datasets:\n",
    "        prediction_obj=build_prediction_object(dataset,detector_obj)\n",
    "\n",
    "        #load dataset file\n",
    "        dataset=pd.read_csv(prediction_obj[\"url\"]) #load dataset\n",
    "        dataset['timestamp']=pd.to_datetime(dataset['timestamp']) #set timestamps to datetime objects\n",
    "\n",
    "        anomalies=build_anomaly_objects(dataset,detector_obj,prediction_obj)\n",
    "\n",
    "        response = await client.request(\"post\",\"/prediction/create\", json={\"payload\":prediction_obj,\"anomalies\":anomalies})\n",
    "\n",
    "    return \"Succesfully added Predictions to all Datasets\"\n",
    "\n",
    "#predictions= await register_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Method: Build All predictions, then build all anomalies for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions={}\n",
    "async def register_anomalies(prediction):\n",
    "    dataset=pd.read_csv(prediction[\"url\"]) #load dataset\n",
    "    dataset['timestamp']=pd.to_datetime(dataset['timestamp']) #set timestamps to datetime objects\n",
    "\n",
    "    anomalies=build_anomaly_objects(dataset,detector_obj,prediction)\n",
    "\n",
    "    if len(anomalies)>1:\n",
    "        anomaly_response = await client.request(\"post\",\"/anomaly/create_many/\"+prediction['id'], json=anomalies)\n",
    "\n",
    "async def generate_predictions():\n",
    "    prediction_objects=[]\n",
    "    for dataset in datasets:\n",
    "        prediction_objects.append(build_prediction_object(dataset,detector_obj))\n",
    "\n",
    "    predictions = await client.request(\"post\",\"/prediction/create_many\", json=prediction_objects)\n",
    "\n",
    "    anomalies = await asyncio.gather(*[register_anomalies(prediction) for prediction in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector in detector_metadata[\"detectors\"]:\n",
    "\n",
    "    detector_payload=detector\n",
    "    MODEL=detector_payload['name']\n",
    "\n",
    "    detector_obj=await client.request(\"post\",\"/detector/create\", json={\"payload\":detector_payload})\n",
    "    predictions= await register_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
